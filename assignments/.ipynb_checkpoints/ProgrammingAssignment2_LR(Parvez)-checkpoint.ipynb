{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "\n",
    "In the linear regression part of this assignment, we have a small dataset available to us. We won't have examples to spare for validation set, instead we'll use cross-validation to tune hyperparameters.\n",
    "\n",
    "### Assignment Goals:\n",
    "In this assignment, we will:\n",
    "* implement linear regression\n",
    "    * use gradient descent for optimization\n",
    "    * implement regularization techniques\n",
    "        * $l_1$/$l_2$ regularization\n",
    "        * use cross-validation to find a good regularization parameter $\\lambda$\n",
    "        \n",
    "### Note:\n",
    "\n",
    "You are not required to follow this exact template. You can change what parameters your functions take or partition the tasks across functions differently. However, make sure there are outputs and implementation for items listed in the rubric for each task. Also, indicate in code with comments which task you are attempting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRADING\n",
    "\n",
    "You will be graded on parts that are marked with **\\#TODO** comments. Read the comments in the code to make sure you don't miss any.\n",
    "\n",
    "### Mandatory for 478 & 878:\n",
    "\n",
    "|   | Tasks                      | 478 | 878 |\n",
    "|---|----------------------------|-----|-----|\n",
    "| 1 | Implement `kfold`          | 20  |  20 |\n",
    "| 2 | Implement `mse`            |  10  |  10  |\n",
    "| 3 | Implement `fit` method     | 40  | 40  |\n",
    "| 4 | Implement `predict` method | 20  | 20  |\n",
    "| 5 | Implement `regularization` | 20  | 10   |\n",
    "\n",
    "### Bonus for 478 & 878\n",
    "|   | Tasks                      | 478 | 878 |\n",
    "|---|----------------------------|-----|-----|\n",
    "| 3 | `fit` (learning rate)       | 10  | 10  |\n",
    "| 6 | Polynomial regression      | 10   | 5   |\n",
    "| 7 | Grid search                | 10  | 5  |\n",
    "\n",
    "Points are broken down further below in Rubric sections. The **first** score is for 478, the **second** is for 878 students. There are a total of 140 points in this part of assignment 2 for 478 and 120 points for 878 students."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use numpy for array operations and matplotlib for plotting for this assignment. Please do not add other libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following code makes the Model class and relevant functions available from \"model.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 'model(Parvez).ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The target value (house prices in $1,000) is plotted against feature values below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "../../../data/housing.data not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mIOError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8b395c1c0504>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../../../data/housing.data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../../../data/housing.names'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'There are {} examples with {} features.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medgecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'k'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a50344dd80cb>\u001b[0m in \u001b[0;36mpreprocess\u001b[0;34m(data_f, feature_names_f)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# e.g. for comma-separated files use delimiter=',' argument.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_f\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/npyio.pyc\u001b[0m in \u001b[0;36mgenfromtxt\u001b[0;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[1;32m   1687\u001b[0m             \u001b[0mfname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1688\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1689\u001b[0;31m             \u001b[0mfhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_datasource\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1690\u001b[0m             \u001b[0mown_fhd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/_datasource.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(path, mode, destpath, encoding, newline)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataSource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnewline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/numpy/lib/_datasource.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, encoding, newline)\u001b[0m\n\u001b[1;32m    614\u001b[0m                                       encoding=encoding, newline=newline)\n\u001b[1;32m    615\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 616\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s not found.\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    617\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: ../../../data/housing.data not found."
     ]
    }
   ],
   "source": [
    "features, feature_names, targets = preprocess('../../data/housing.data', '../../data/housing.names')\n",
    "print('There are {} examples with {} features.'.format(features.shape[0], features.shape[1]))\n",
    "print(targets.shape[0])\n",
    "%matplotlib inline\n",
    "fig, axs = plt.subplots(4, 4, figsize=(15, 15), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.2, wspace=.20)\n",
    "\n",
    "# DISREGARD LAST 3 EMPTY PLOTS\n",
    "for index, feature_name in enumerate(feature_names):\n",
    "    \n",
    "    axs[index//4][index %4].scatter(features[:, index], targets)\n",
    "    axs[index//4][index %4].set_xlabel(feature_name)\n",
    "\n",
    "fig.text(0.06, 0.5, 'House Value in $1000', ha='center', va='center', rotation='vertical', size=24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 1: Implement `kfold`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement \"kfold\" function for $k$-fold cross-validation in \"model.ipynb\". 5 and 10 are commonly used values for $k$. You can use either one of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric:\n",
    " * No intersection between test and train parts +10, +10\n",
    " * No intersection between test folds +10, +10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `kfold`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain 5 splits of data.\n",
    "splits = kfold(targets.shape[0], k=5)\n",
    "\n",
    "\n",
    "# Check that test folds are completely different\n",
    "# Check that for a given i, train and test are completely different\n",
    "for i in range(5):\n",
    "    intersection = set(splits[i][0]).intersection (set(splits[i][1]))\n",
    "    if intersection:\n",
    "        print('Test-train splits intersect!')\n",
    "    for j in range(5):\n",
    "        if i!=j:\n",
    "            intersection = set(splits[i][1]).intersection (set(splits[j][1]))\n",
    "            if intersection:\n",
    "                print('Test splits intersect!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 2: Implement `mse`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll  use mean squared error (mse) for linear regression. Next, implement \"mse\" function in \"model.ipynb\" that takes predicted and true target values, and returns the \"mse\" between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric:\n",
    " * Correct mse +10, +10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test `mse`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5000.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(np.array([100, 300]), np.array([200, 400]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASKS 3, 4, 5: Implement `fit`, `predict`, `regularization`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define our linear_regression model class now. Implement the \"fit\" and \"predict\" methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric:\n",
    "* fit without regularization +20, +20\n",
    "* learning rate interpretation +10, +10 (BONUS for both)\n",
    "* $l_1$ regularization +10, +5\n",
    "* $l_2$ regularization +10, +5\n",
    "* fit works with regularization +20, +20\n",
    "* predict +20, +20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression(Model):\n",
    "    Theta=[]  \n",
    "    # You can disregard regularizer and kwargs for TASK 3\n",
    "    def fit(self, X, Y, learning_rate = 0.1, epochs = 2000, regularizer=None, lambd=0,**kwargs):\n",
    "        '''\n",
    "        Args: \n",
    "            learning_rate: float\n",
    "                step size for parameter update\n",
    "            epochs: int\n",
    "                number of updates that will be performed\n",
    "            regularizer: str\n",
    "                one of l1 or l2\n",
    "            lambd: float\n",
    "                regularization coefficient\n",
    "        '''\n",
    "\n",
    "        # we will need to add a column of 1's for bias\n",
    "        \n",
    "        \n",
    "        size = X.shape[0]\n",
    "        ones = np.random.rand(size)\n",
    "        ones = np.reshape(ones, (size ,-1))\n",
    "        features = np.hstack((ones, X))\n",
    "        #features.astype(int)\n",
    "        \n",
    "        \n",
    "        # theta_hat contains the parameters for the model\n",
    "        # initialize theta_hat as zeros\n",
    "        # one parameter for each feature and one for bias\n",
    "        theta_hat = np.zeros(X.shape[1])\n",
    "        \n",
    "        theta_hat_with_bias=np.zeros(features.shape[1])\n",
    "        #theta_with_bias.astype(int)\n",
    "       \n",
    "        # TODO\n",
    "        \n",
    "        # for each epoch\n",
    "        for epoch in range(epochs):\n",
    "            # compute model predictions for training examples\n",
    "            y_hat =np.dot(features,theta_hat_with_bias)\n",
    "            \n",
    "            if regularizer == None:\n",
    "\n",
    "                # use mse function to find the cost\n",
    "                cost = mse(y_hat, Y)\n",
    "                \n",
    "                \n",
    "                # You can use below print statement to monitor cost\n",
    "                #print('Current cost is {}'.format(cost))\n",
    "                # calculate gradients wrt theta\n",
    "                grad_theta = np.transpose(features).dot(features.dot(theta_hat_with_bias)-Y)\n",
    "                # update theta\n",
    "                theta_hat_with_bias = theta_hat_with_bias - ((learning_rate/size)*grad_theta)\n",
    "                \n",
    "\n",
    "            elif regularizer =='l2':\n",
    "                # take regularization into account\n",
    "                # use your regularization function\n",
    "                # you will need to compute the gradient of the regularization term\n",
    "                reg_theta=theta_hat_with_bias\n",
    "                reg_theta[0]=1\n",
    "                # use mse function to find the cost\n",
    "                cost = mse(y_hat, Y)\n",
    "                \n",
    "                \n",
    "                # You can use below print statement to monitor cost\n",
    "                #print('Current cost is {}'.format(cost))\n",
    "                # calculate gradients wrt theta\n",
    "                grad_theta = np.transpose(features).dot(features.dot(theta_hat_with_bias)-Y)\n",
    "                # update theta\n",
    "                theta_hat_with_bias = theta_hat_with_bias - ((learning_rate/size)*grad_theta)-(learning_rate*lambd*reg_theta/size)\n",
    "                                   \n",
    "                \n",
    "            elif regularizer =='l1':\n",
    "                # take regularization into account\n",
    "                # use your regularization function\n",
    "                # you will need to compute the gradient of the regularization term\n",
    "                #reg_theta=np.delete(theta_hat_with_bias,0,0)\n",
    "                reg_theta=theta_hat_with_bias\n",
    "                reg_theta[0]=0\n",
    "                # use mse function to find the cost\n",
    "                cost = mse(y_hat, Y)\n",
    "                \n",
    "                \n",
    "                # You can use below print statement to monitor cost\n",
    "                #print('Current cost is {}'.format(cost))\n",
    "                # calculate gradients wrt theta\n",
    "                grad_theta = np.transpose(features).dot(features.dot(theta_hat_with_bias)-Y)\n",
    "                # update theta\n",
    "                theta_hat_with_bias = theta_hat_with_bias - ((learning_rate/size)*grad_theta)-(learning_rate*lambd*np.sin(reg_theta)/size)\n",
    "                                   \n",
    "        # update the model parameters to be used in predict method\n",
    "        self.theta = theta_hat_with_bias\n",
    "        #Theta = theta_hat_with_bias\n",
    "        #print(self.theta)\n",
    "        \n",
    "    def predict(self, test_features):\n",
    "        \n",
    "        # obtain test features for current fold\n",
    "        # do not forget to add a column for bias\n",
    "        # as in fit method\n",
    "        \n",
    "        size = test_features.shape[0]\n",
    "        ones = np.ones(size)\n",
    "        ones = np.reshape(ones, (size ,-1))\n",
    "        test_features = np.hstack((ones, test_features))\n",
    "\n",
    "        # TODO\n",
    "        \n",
    "        \n",
    "        \n",
    "        # get predictions from model\n",
    "        #print(self.theta)\n",
    "        y_hat = np.dot(test_features,self.theta)\n",
    "        #y_hat = np.dot(test_features,Theta)\n",
    "\n",
    "        return y_hat\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize and fit the model. During training monitor your cost function. Experiment with different learning rates. Insert a cell below and summarize and briefly interpret your observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.00548707 -0.05865178  0.12721816 -0.03514933  0.00410716  0.00188264\n",
      "  0.06728052  0.05812639  0.00965126  0.01078945  0.00120812  0.03891179\n",
      "  0.05133657 -0.21529589]\n"
     ]
    }
   ],
   "source": [
    "# initialize and fit the model\n",
    "my_model = Linear_Regression()\n",
    "# change lr to try different learning rates\n",
    "lr = 0.000005\n",
    "my_model.fit(features[splits[0][0]], targets[splits[0][0]], learning_rate = lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define \"regularization\" function which implements $l_1$ and $l_2$ regularization in \"model.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.500000000000001\n",
      "3.8500000000000005\n"
     ]
    }
   ],
   "source": [
    "weights = list(np.arange(0, 1.1 , 0.1))\n",
    "for method in ['l1', 'l2']:\n",
    "    print(regularization(weights, method=method))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 6: Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the dataset would benefit from polynomial regression? Please briefly explain why or why not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric:\n",
    "* Sound reasoning +10, +5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TASK 7: Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using cross-validation, try different values of $\\lambda$ for $l_1$ and $l_2$ regularization to find good $\\lambda$ values that result in low average _mse_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rubric:\n",
    "* Different methods are tried with different values of $\\lambda$ +10, +5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test: Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Linear_Regression' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-38aa9268695c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# initialize the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinear_Regression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# two regularization methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'l1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'l2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Linear_Regression' is not defined"
     ]
    }
   ],
   "source": [
    "# initialize the model\n",
    "my_model = Linear_Regression()\n",
    "print(np.arange(0, 1, 0.1))\n",
    "# two regularization methods\n",
    "for method in ['l1', 'l2']:\n",
    "    # different lambda\n",
    "    for lmbd in np.arange(0, 1, 0.1):\n",
    "        \n",
    "        k_fold_mse = 0\n",
    "        fit_kwargs={'method': method}\n",
    "        \n",
    "        for k in range(5):\n",
    "            \n",
    "            # fit on training\n",
    "            my_model.fit(features[splits[k][0]], targets[splits[k][0]], learning_rate = 0.0000001, regularizer=method, lambd = lmbd)\n",
    "            # predict test\n",
    "            pred = my_model.predict(features[splits[k][1]])\n",
    "            k_fold_mse += mse(pred,targets[splits[k][1]])\n",
    "        print(k_fold_mse/5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
